#+TITLE: Makemore [[[https://www.youtube.com/watch?v=PaCmpygFfXo][YT Video]]]
#+DESCRIPTION:  The spelled-out intro to language modelling: character-level autoregressive model
#+PROPERTY: header-args:jupyter-python :session makemore :kernel python3 :async yes

* Dataset
#+begin_src sh
mkdir -p datasets
curl https://raw.githubusercontent.com/karpathy/makemore/master/names.txt > datasets/names.txt
#+end_src

#+RESULTS:

** Exploring the dataset
#+begin_src jupyter-python
words = open('datasets/names.txt').read().splitlines()
#+end_src

#+RESULTS:

#+begin_src jupyter-python
len(words)
#+end_src

#+RESULTS:
: 32033

** Finding all bigrams
#+begin_src jupyter-python
from collections import Counter
bigrams = Counter()

for word in words:
    word = ['<S>'] + list(word) + ['<E>']
    bigrams.update(zip(word, word[1:]))
#+end_src

#+RESULTS:

#+begin_src jupyter-python
bigrams.most_common(10)
#+end_src

#+RESULTS:
| (n <E>) | 6763 |
| (a <E>) | 6640 |
| (a n)   | 5438 |
| (<S> a) | 4410 |
| (e <E>) | 3983 |
| (a r)   | 3264 |
| (e l)   | 3248 |
| (r i)   | 3033 |
| (n a)   | 2977 |
| (<S> k) | 2963 |

** From finding, to storing
#+begin_src jupyter-python
import torch
#+end_src

#+RESULTS:

*** Building the language
#+begin_src jupyter-python
DEMARKATOR = '|'
LANGUAGE = [DEMARKATOR] + sorted(list(set(''.join(words))))

idx_map = {v: k for (k, v) in enumerate(LANGUAGE)}
char_map = {k: v for (k, v) in enumerate(LANGUAGE)}
#+end_src

#+RESULTS:

*** Building the array, in PyTorch
#+begin_src jupyter-python
bigrams = torch.zeros((len(LANGUAGE), len(LANGUAGE)), dtype = torch.int32)

for word in words:
    word = DEMARKATOR + word + DEMARKATOR
    for bigram in zip(word, word[1:]):
        bigram_idx = [idx_map[ch] for ch in bigram]
        bigrams[bigram_idx[0], bigram_idx[1]] += 1
#+end_src

#+RESULTS:

#+begin_src jupyter-python
bigrams
#+end_src

#+RESULTS:
#+begin_example
tensor([[   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,
         1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,
          134,  535,  929],
        [6640,  556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568,
         2528, 1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,
          182, 2050,  435],
        [ 114,  321,   38,    1,   65,  655,    0,    0,   41,  217,    1,    0,
          103,    0,    4,  105,    0,    0,  842,    8,    2,   45,    0,    0,
            0,   83,    0],
        [  97,  815,    0,   42,    1,  551,    0,    2,  664,  271,    3,  316,
          116,    0,    0,  380,    1,   11,   76,    5,   35,   35,    0,    0,
            3,  104,    4],
        [ 516, 1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,
           60,   30,   31,  378,    0,    1,  424,   29,    4,   92,   17,   23,
            0,  317,    1],
        [3983,  679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178,
         3248,  769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,
          132, 1070,  181],
        [  80,  242,    0,    0,    0,  123,   44,    1,    1,  160,    0,    2,
           20,    0,    4,   60,    0,    0,  114,    6,   18,   10,    0,    4,
            0,   14,    2],
        [ 108,  330,    3,    0,   19,  334,    1,   25,  360,  190,    3,    0,
           32,    6,   27,   83,    0,    0,  201,   30,   31,   85,    1,   26,
            0,   31,    1],
        [2409, 2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,
          185,  117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,
            0,  213,   20],
        [2489, 2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445,
         1345,  427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,
           89,  779,  277],
        [  71, 1473,    1,    4,    4,  440,    0,    0,   45,  119,    2,    2,
            9,    5,    2,  479,    1,    0,   11,    7,    2,  202,    5,    6,
            0,   10,    0],
        [ 363, 1731,    2,    2,    2,  895,    1,    0,  307,  509,    2,   20,
          139,    9,   26,  344,    0,    0,  109,   95,   17,   50,    2,   34,
            0,  379,    2],
        [1314, 2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24,
         1345,   60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,
            0, 1588,   10],
        [ 516, 2590,  112,   51,   24,  818,    1,    0,    5, 1256,    7,    1,
            5,  168,   20,  452,   38,    0,   97,   35,    4,  139,    3,    2,
            0,  287,   11],
        [6763, 2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,
          195,   19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,
            6,  465,  145],
        [ 855,  149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,
          619,  261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,
           45,  103,   54],
        [  33,  209,    2,    1,    0,  197,    1,    0,  204,   61,    1,    1,
           16,    1,    1,   59,   39,    0,  151,   16,   17,    4,    0,    0,
            0,   12,    0],
        [  28,   13,    0,    0,    0,    1,    0,    0,    0,   13,    0,    0,
            1,    2,    0,    2,    0,    0,    1,    2,    0,  206,    0,    3,
            0,    0,    0],
        [1377, 2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,
          413,  162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,
            3,  773,   23],
        [1169, 1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,
          279,   90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,
            0,  215,   10],
        [ 483, 1027,    1,   17,    0,  716,    2,    2,  647,  532,    3,    0,
          134,    4,   22,  667,    0,    0,  352,   35,  374,   78,   15,   11,
            2,  341,  105],
        [ 155,  163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,
          301,  154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,
           34,   13,   45],
        [  88,  642,    1,    0,    1,  568,    0,    0,    1,  911,    0,    3,
           14,    0,    8,  153,    0,    0,   48,    0,    0,    7,    7,    0,
            0,  121,    0],
        [  51,  280,    1,    0,    8,  149,    2,    1,   23,  148,    0,    6,
           13,    2,   58,   36,    0,    0,   22,   20,    8,   25,    0,    2,
            0,   73,    1],
        [ 164,  103,    1,    4,    5,   36,    3,    0,    1,  102,    0,    0,
           39,    1,    1,   41,    0,    0,    0,   31,   70,    5,    0,    3,
           38,   30,   19],
        [2007, 2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86,
         1104,  148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,
           28,   23,   78],
        [ 160,  860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,
          123,   35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,
            1,  147,   45]], dtype=torch.int32)
#+end_example

*** Visualising the tensor
#+begin_src jupyter-python  :results graphics file output :file plots/bigram-plot.png
import matplotlib.pyplot as plt

plt.figure(figsize=(16, 16))
plt.imshow(bigrams, cmap='Reds')
for i in range(len(bigrams)):
    for j in range(len(bigrams[i])):
        bigram = char_map[i] + char_map[j]
        plt.text(j, i, bigram, ha="center", va="bottom", color="gray")
        plt.text(j, i, bigrams[i, j].item(), ha="center", va="top", color="gray")

plt.axis('off')
plt.plot()
#+end_src

#+RESULTS:
[[file:plots/bigram-plot.png]]

* Sampling, the basics
Personally, I would have gone the Bayesian approach (assume each bigram appears normally in the dataset, and then generate posteriors using those priors). But, who am I to judge? It would be an interesting thing to look at once I return to this afterwards. For now, however, let's just stick to Andrej's version :)

** PyTorch specific stuff
1. =squeeze=: squeezes array into smallest dimension (example: =[[1, 2]]= turns to =[1, 2]=)
2. =torch.sum()= can sum across multiple dimensions.
3. [[https://pytorch.org/docs/stable/notes/broadcasting.html][Broadcasting semantics]] in PyTorch is important! Used to determine how to "broadcast" inline operations in PyTorch across tensors
   1. Idea:
      1. Align all dimensions to the _right_
      2. From the right, move leftwards, ensuring that all dimensions have the following properties:
	 1. All dimensions are the same
	 2. One dimension is one
	 3. The dimension does not exist
   2. Example bug (See the video at [[https://youtu.be/PaCmpygFfXo?t=2640][~45 mins]])
      #+begin_src jupyter-python
      P = bigrams.float()
      P = P / P.sum(1) # sums rows
      #             ^--------------------- bug here: dimension will be (27,)
      #+end_src

      =P.shape= will be $(27, 27)$, while the sum will be a vector of length 27, i.e. shape $(27, )$. Therefore, according to the broadcasting rules, the broadcasted operation will divide the probabilities of each row across each column!
      #+begin_example
      P.shape        27 27
      row sum.shape     27
      #+end_example

** Naive way
#+begin_src jupyter-python
bigram_probs = bigrams.float()
bigram_probs = bigram_probs / bigram_probs.sum(1, keepdim=True)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
def generate_name (bigram_probs, rng):
    name = []
    idx = 0
    while True:
        p = bigram_probs[idx]
        idx = torch.multinomial(p, num_samples = 1, replacement = True, generator = rng).item()

        if idx == 0:
            break

        name += char_map[idx]

    return ''.join(name)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
rng = torch.Generator().manual_seed((2 << 30) - 1)

num_names = 10
print([generate_name(bigram_probs, rng) for _ in range(num_names)])
#+end_src

#+RESULTS:
: ['junide', 'janasah', 'p', 'cony', 'a', 'nn', 'kohin', 'tolian', 'juee', 'ksahnaauranilevias']

* Modelling Loss
Earlier, we had a straightforward way of modelling loss, since we had a test and train dataset that had numerical values. But how do we do that with language, where there are no numbers?

We can think of our model as a predictor, and what we want is the predictions to be as close as possible to the actual thing. How do we predict things? Using the probabiliies. Infact, the probability of generating a certain word is merely the product of the probabilities of the respective bigrams. This is basically a Maximum Likliehood Estimator in disguise.
$$
P(\text{word}) = \prod_{n = 0}^{\text{length}} P(\text{bigram})
$$

We know that we want the probability of generation to be as close to 1 as possible, since that way it would generate only that word. A naive loss function, therefore, would merely be the product. We can also take the negative log likelihood (the negative is merely to turn the function from negative to positive). However, this has a slight disadvantage: it penalises long names. To counter this, one can take the average loss per character.

#+begin_src jupyter-python
def loss_function (bigram_probs, word):
    word = [DEMARKATOR] + list(word) + [DEMARKATOR]
    log_likelihood = 0.0
    for ch1, ch2 in zip(word, word[1:]):
        log_likelihood += torch.log(bigram_probs[idx_map[ch1], idx_map[ch2]])
    return -log_likelihood / (len(word) - 1)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
loss_function(bigram_probs, generate_name(bigram_probs, rng))
#+end_src

#+RESULTS:
: tensor(2.1849)

** Fixing a minor problem
Here, there are some values with zero counts. This is dangerous for our loss function, because it might blow up the log function to =-inf=!
#+begin_src jupyter-python
torch.log(torch.tensor(0.0))
#+end_src

#+RESULTS:
: tensor(-inf)

In fact, it is easy to see this using some weird bigrams, like =qq=.
#+begin_src jupyter-python
loss_function(bigram_probs, "qq")
#+end_src

#+RESULTS:
: tensor(inf)

The answer: smoothen all values! We can add some small value to begin with, and count all occurences using that. This, as it turns out, is also known as [[https://en.wikipedia.org/wiki/Additive_smoothing][additive, or Laplacian smoothening]].
#+begin_src jupyter-python
smooth_bigram_probs = (bigrams + 1e-6).float()
smooth_bigram_probs = smooth_bigram_probs / smooth_bigram_probs.sum(1, keepdim=True)

loss_function(smooth_bigram_probs, "qq")
#+end_src

#+RESULTS:
: tensor(9.1825)

Note: This is basically creating a problem and fixing it, patting oneself on the back while doing so. Bayesian methods would make this update pointless, since every point always has some probability, be it however small.
* Chucking an NN at the problem. Problem solved?
Let's find out, shall we? :)
** Finding all bigrams
#+begin_src jupyter-python
def get_bigrams (word):
    word = [DEMARKATOR] + list(word) + [DEMARKATOR]
    for bigram in zip(word, word[1:]):
        yield bigram
#+end_src

#+RESULTS:

** Creating a one-hot encoding
#+begin_src jupyter-python
bigram_idxs = [[idx_map[ch] for ch in bigram] for word in words for bigram in get_bigrams(word)]
xs, ys = zip(*bigram_idxs)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
import torch.nn.functional as F

# converts a vector into a one-hot encoding
one_hot_encoding = lambda x: F.one_hot(torch.tensor(x), num_classes=len(LANGUAGE)).float() 

x_encoding = one_hot_encoding(xs)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
x_encoding.shape
#+end_src

#+RESULTS:
: torch.Size([228146, 27])

#+begin_src jupyter-python :results graphics file output :file plots/one-hot-visual.png
plt.imshow(x_encoding)
#+end_src

#+RESULTS:
[[file:plots/one-hot-visual.png]]

** Actual NN

#+begin_src jupyter-python
W = torch.randn((27, 27), generator=rng, requires_grad=True)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
W.shape
#+end_src

#+RESULTS:
: torch.Size([27, 27])

#+begin_src jupyter-python
num = len(xs)

for k in range(100):
    # forward pass
    logits = x_encoding @ W
    counts = logits.exp()
    probs = counts / counts.sum(1, keepdim = True) # probs = logits.softmax()
    loss = -probs[torch.arange(num), ys].log().mean()
    print(f"{k=}, {loss.value()}")

    # backward pass
    W.grad = None
    loss.backward()

    # gradient update
    W.data += -20 * W.grad
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: [0;31m---------------------------------------------------------------------------[0m
: [0;31mNameError[0m                                 Traceback (most recent call last)
: Cell [0;32mIn[2], line 1[0m
: [0;32m----> 1[0m num [38;5;241m=[39m [38;5;28mlen[39m([43mxs[49m)
: [1;32m      3[0m [38;5;28;01mfor[39;00m k [38;5;129;01min[39;00m [38;5;28mrange[39m([38;5;241m100[39m):
: [1;32m      4[0m     [38;5;66;03m# forward pass[39;00m
: [1;32m      5[0m     logits [38;5;241m=[39m x_encoding [38;5;241m@[39m W
: 
: [0;31mNameError[0m: name 'xs' is not defined
:END:

** Regularisation: The last bit
Regularisation is actually a very cool idea. It's main goal is to bring generality and robustness in neural networks.

The core idea behind regularlisation is to add an implicit or explicity regularisation term in the loss function, that forces the paramters to become "simpler". By preventing weird stuff like 0.0000000024892 instead of 0, it allows the neural network to be represented in simpler terms, and therefore, generalises better (or atleast, that is the aim).

Explicit methods are usually the addition of $L_{1, 2}$ terms to the loss function.

* MLP: Upgrading our simple neural network
** Rebuilding the dataset
#+begin_src jupyter-python
import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn.functional as F
#+end_src

#+RESULTS:

#+begin_src jupyter-python
DEMARKATOR = '|'
LANGUAGE = [DEMARKATOR] + sorted(list(set(''.join(words))))

idx_map = {v: k for (k, v) in enumerate(LANGUAGE)}
char_map = {k: v for (k, v) in enumerate(LANGUAGE)}
#+end_src

#+RESULTS:

#+begin_src jupyter-python
BLOCK_SIZE = 3

xs = []
ys = []

for word in words:
    word += DEMARKATOR
    padded_word = DEMARKATOR * BLOCK_SIZE + word
    word_grams = [padded_word[i:i + BLOCK_SIZE] for i in range(len(word))]
    word_predictions = [(word[i], ch) for (i, ch) in enumerate(word_grams)]

    y, x = list(zip(*word_predictions))
    x = [[idx_map[ch] for ch in x_i] for x_i in x]
    y = [idx_map[ch] for ch in y]

    xs.extend(x)
    ys.extend(y)

X = torch.tensor(xs)
Y = torch.tensor(ys)
#+end_src

#+RESULTS:
** Building our embeddings
Fun fact: we can index PyTorch tensors using all sorts of weird things. Including...(dromroll)...tensors, themselves. Yes, indexing a tensor using a tensor is valid. Therefore, all of that fiddling we did with the one-hot encoding earlier is, well, built into PyTorch.

#+begin_src jupyter-python
C = torch.randn((27, 2))

emb = C[X]
#+end_src

#+RESULTS:
** PyTorch funsies
PyTorch tensors are very flexible objects. To concatenate the embeddings of a block, one might naively concatenate all of the embeddings as follows:
#+begin_src jupyter-python
torch.cat(torch.unbind(emb, 1), 1).shape
#+end_src

#+RESULTS:
: torch.Size([16, 6])

This works perfectly fine, except for a small problem: it creates an entirely new tensor for achieving the same thing. This is rather wasteful. Fortunately, PyTorch provides for a way to rephrase our sentiment:

#+begin_src jupyter-python
emb.view(emb.shape[0], 6)
#+end_src

#+RESULTS:
#+begin_example
tensor([[-0.4726, -0.5054, -0.4726, -0.5054, -0.4726, -0.5054],
        [-0.4726, -0.5054, -0.4726, -0.5054,  0.2323, -0.0498],
        [-0.4726, -0.5054,  0.2323, -0.0498, -0.0745, -0.6577],
        [ 0.2323, -0.0498, -0.0745, -0.6577, -0.0745, -0.6577],
        [-0.0745, -0.6577, -0.0745, -0.6577, -0.7213, -0.7095],
        [-0.4726, -0.5054, -0.4726, -0.5054, -0.4726, -0.5054],
        [-0.4726, -0.5054, -0.4726, -0.5054, -1.4461,  0.0427],
        [-0.4726, -0.5054, -1.4461,  0.0427,  1.1246, -0.7019],
        [-1.4461,  0.0427,  1.1246, -0.7019, -0.4193, -1.6884],
        [ 1.1246, -0.7019, -0.4193, -1.6884,  1.3284, -0.5544],
        [-0.4193, -1.6884,  1.3284, -0.5544, -0.4193, -1.6884],
        [ 1.3284, -0.5544, -0.4193, -1.6884, -0.7213, -0.7095],
        [-0.4726, -0.5054, -0.4726, -0.5054, -0.4726, -0.5054],
        [-0.4726, -0.5054, -0.4726, -0.5054, -0.7213, -0.7095],
        [-0.4726, -0.5054, -0.7213, -0.7095,  1.3284, -0.5544],
        [-0.7213, -0.7095,  1.3284, -0.5544, -0.7213, -0.7095]])
#+end_example

or, alternatively:
#+begin_src jupyter-python
emb.view(-1, 6).shape
#+end_src

#+RESULTS:
: torch.Size([16, 6])
where the -1 suggests PyTorch to inherit the dimension from the original element
** Cross Entropy
As it turns out, how we calculate the loss function is well known as the cross entropy loss.

More accurately, cross entropy $\mathcal{H}(p, q)$ is given by
$$
\mathcal{H}(p, q) = -\sum_{x} p(x) \log q(x)
$$

where $p(x)$ is the ideal distribution, and $q(x)$ is the estimated distribution. In this case, since we only have an estimator of the actual distribution of letters, the cross entropy loss can also be calcualted as
$$
\mathcal{H}(q) = -\sum_{x} \frac1{N} \log q(x)
$$
which is pretty much what we are doing. This interpretation is a Monte Carlo estimator for the cross entropy loss.
** Setting up the neural network correctly
We can setup up our neural network using the following basics.

#+begin_src jupyter-python
class Params:
    def __init__(self, C, W1, b1, W2, b2):
        self.C = C
        self.W1 = W1
        self.b1 = b1
        self.W2 = W2
        self.b2 = b2

        self.__params__ = [C, W1, b1, W2, b2]

    def __iter__(self):
        return self.__params__.__iter__()

    def __next__(self):
        return self.__params__.__next__()
#+end_src

#+RESULTS:

#+begin_src jupyter-python
def setup(rng, embedding_dim = 2, layer_width=100):
    C = torch.randn((27, embedding_dim), generator=rng, requires_grad=True)
    W1 = torch.randn((embedding_dim * BLOCK_SIZE, layer_width), generator=rng, requires_grad=True)
    b1 = torch.randn((layer_width,), generator=rng, requires_grad=True)
    W2 = torch.randn((layer_width, len(LANGUAGE)), generator=rng, requires_grad=True)
    b2 = torch.randn((len(LANGUAGE),), generator=rng, requires_grad=True)
    
    params = Params(C, W1, b1, W2, b2)
    return params

def forward_pass (p, X, embedding_dim = 2):
    emb = p.C[X]
    h = emb.view(-1, embedding_dim * BLOCK_SIZE) @ p.W1 + p.b1
    h = torch.tanh(h)
    logits = h @ p.W2 + p.b2
    return logits

def backward_pass (params, loss, lr = 0.1):
    for p in params:
        p.grad = None

    loss.backward()

    for p in params:
        p.data -= lr * p.grad
#+end_src

#+RESULTS:

#+begin_src jupyter-python
rng = torch.Generator().manual_seed(42)
params = setup(rng)

NUM_RUNS = 1000

for i in range(NUM_RUNS):
    logits = forward_pass(params, X)
    loss = F.cross_entropy(logits, Y)
    if (i % 100 == 0):
        print(loss.item(), end='\r')
    backward_pass(params, loss, 0.1)
#+end_src

#+RESULTS:
: 2.6900081634521484

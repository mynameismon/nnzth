#+TITLE: Makemore [[[https://www.youtube.com/watch?v=PaCmpygFfXo][YT Video]]]
#+DESCRIPTION:  The spelled-out intro to language modelling: character-level autoregressive model
#+PROPERTY: header-args:jupyter-python :session makemore :kernel python3 :async yes

* Dataset
#+begin_src sh
mkdir -p datasets
curl https://raw.githubusercontent.com/karpathy/makemore/master/names.txt > datasets/names.txt
#+end_src

#+RESULTS:

** Exploring the dataset
#+begin_src jupyter-python
words = open('datasets/names.txt').read().splitlines()
#+end_src

#+RESULTS:

#+begin_src jupyter-python
len(words)
#+end_src

#+RESULTS:
: 32033

** Finding all bigrams
#+begin_src jupyter-python
from collections import Counter
bigrams = Counter()

for word in words:
    word = ['<S>'] + list(word) + ['<E>']
    bigrams.update(zip(word, word[1:]))
#+end_src

#+RESULTS:

#+begin_src jupyter-python
bigrams.most_common(10)
#+end_src

#+RESULTS:
| (n <E>) | 6763 |
| (a <E>) | 6640 |
| (a n)   | 5438 |
| (<S> a) | 4410 |
| (e <E>) | 3983 |
| (a r)   | 3264 |
| (e l)   | 3248 |
| (r i)   | 3033 |
| (n a)   | 2977 |
| (<S> k) | 2963 |

** From finding, to storing
#+begin_src jupyter-python
import torch
#+end_src

#+RESULTS:

*** Building the language
#+begin_src jupyter-python
DEMARKATOR = '|'
LANGUAGE = [DEMARKATOR] + sorted(list(set(''.join(words))))

idx_map = {v: k for (k, v) in enumerate(LANGUAGE)}
char_map = {k: v for (k, v) in enumerate(LANGUAGE)}
#+end_src

#+RESULTS:

*** Building the array, in PyTorch
#+begin_src jupyter-python
bigrams = torch.zeros((len(LANGUAGE), len(LANGUAGE)), dtype = torch.int32)

for word in words:
    word = DEMARKATOR + word + DEMARKATOR
    for bigram in zip(word, word[1:]):
        bigram_idx = [idx_map[ch] for ch in bigram]
        bigrams[bigram_idx[0], bigram_idx[1]] += 1
#+end_src

#+RESULTS:

#+begin_src jupyter-python
bigrams
#+end_src

#+RESULTS:
#+begin_example
tensor([[   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,
         1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,
          134,  535,  929],
        [6640,  556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568,
         2528, 1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,
          182, 2050,  435],
        [ 114,  321,   38,    1,   65,  655,    0,    0,   41,  217,    1,    0,
          103,    0,    4,  105,    0,    0,  842,    8,    2,   45,    0,    0,
            0,   83,    0],
        [  97,  815,    0,   42,    1,  551,    0,    2,  664,  271,    3,  316,
          116,    0,    0,  380,    1,   11,   76,    5,   35,   35,    0,    0,
            3,  104,    4],
        [ 516, 1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,
           60,   30,   31,  378,    0,    1,  424,   29,    4,   92,   17,   23,
            0,  317,    1],
        [3983,  679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178,
         3248,  769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,
          132, 1070,  181],
        [  80,  242,    0,    0,    0,  123,   44,    1,    1,  160,    0,    2,
           20,    0,    4,   60,    0,    0,  114,    6,   18,   10,    0,    4,
            0,   14,    2],
        [ 108,  330,    3,    0,   19,  334,    1,   25,  360,  190,    3,    0,
           32,    6,   27,   83,    0,    0,  201,   30,   31,   85,    1,   26,
            0,   31,    1],
        [2409, 2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,
          185,  117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,
            0,  213,   20],
        [2489, 2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445,
         1345,  427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,
           89,  779,  277],
        [  71, 1473,    1,    4,    4,  440,    0,    0,   45,  119,    2,    2,
            9,    5,    2,  479,    1,    0,   11,    7,    2,  202,    5,    6,
            0,   10,    0],
        [ 363, 1731,    2,    2,    2,  895,    1,    0,  307,  509,    2,   20,
          139,    9,   26,  344,    0,    0,  109,   95,   17,   50,    2,   34,
            0,  379,    2],
        [1314, 2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24,
         1345,   60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,
            0, 1588,   10],
        [ 516, 2590,  112,   51,   24,  818,    1,    0,    5, 1256,    7,    1,
            5,  168,   20,  452,   38,    0,   97,   35,    4,  139,    3,    2,
            0,  287,   11],
        [6763, 2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,
          195,   19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,
            6,  465,  145],
        [ 855,  149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,
          619,  261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,
           45,  103,   54],
        [  33,  209,    2,    1,    0,  197,    1,    0,  204,   61,    1,    1,
           16,    1,    1,   59,   39,    0,  151,   16,   17,    4,    0,    0,
            0,   12,    0],
        [  28,   13,    0,    0,    0,    1,    0,    0,    0,   13,    0,    0,
            1,    2,    0,    2,    0,    0,    1,    2,    0,  206,    0,    3,
            0,    0,    0],
        [1377, 2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,
          413,  162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,
            3,  773,   23],
        [1169, 1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,
          279,   90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,
            0,  215,   10],
        [ 483, 1027,    1,   17,    0,  716,    2,    2,  647,  532,    3,    0,
          134,    4,   22,  667,    0,    0,  352,   35,  374,   78,   15,   11,
            2,  341,  105],
        [ 155,  163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,
          301,  154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,
           34,   13,   45],
        [  88,  642,    1,    0,    1,  568,    0,    0,    1,  911,    0,    3,
           14,    0,    8,  153,    0,    0,   48,    0,    0,    7,    7,    0,
            0,  121,    0],
        [  51,  280,    1,    0,    8,  149,    2,    1,   23,  148,    0,    6,
           13,    2,   58,   36,    0,    0,   22,   20,    8,   25,    0,    2,
            0,   73,    1],
        [ 164,  103,    1,    4,    5,   36,    3,    0,    1,  102,    0,    0,
           39,    1,    1,   41,    0,    0,    0,   31,   70,    5,    0,    3,
           38,   30,   19],
        [2007, 2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86,
         1104,  148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,
           28,   23,   78],
        [ 160,  860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,
          123,   35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,
            1,  147,   45]], dtype=torch.int32)
#+end_example

*** Visualising the tensor
#+begin_src jupyter-python  :results graphics file output :file plots/bigram-plot.png
import matplotlib.pyplot as plt

plt.figure(figsize=(16, 16))
plt.imshow(bigrams, cmap='Reds')
for i in range(len(bigrams)):
    for j in range(len(bigrams[i])):
        bigram = char_map[i] + char_map[j]
        plt.text(j, i, bigram, ha="center", va="bottom", color="gray")
        plt.text(j, i, bigrams[i, j].item(), ha="center", va="top", color="gray")

plt.axis('off')
plt.plot()
#+end_src

#+RESULTS:
[[file:plots/bigram-plot.png]]

* Sampling, the basics
Personally, I would have gone the Bayesian approach (assume each bigram appears normally in the dataset, and then generate posteriors using those priors). But, who am I to judge? It would be an interesting thing to look at once I return to this afterwards. For now, however, let's just stick to Andrej's version :)

** PyTorch specific stuff
1. =squeeze=: squeezes array into smallest dimension (example: =[[1, 2]]= turns to =[1, 2]=)
2. =torch.sum()= can sum across multiple dimensions.
3. [[https://pytorch.org/docs/stable/notes/broadcasting.html][Broadcasting semantics]] in PyTorch is important! Used to determine how to "broadcast" inline operations in PyTorch across tensors
   1. Idea:
      1. Align all dimensions to the _right_
      2. From the right, move leftwards, ensuring that all dimensions have the following properties:
	 1. All dimensions are the same
	 2. One dimension is one
	 3. The dimension does not exist
   2. Example bug (See the video at [[https://youtu.be/PaCmpygFfXo?t=2640][~45 mins]])
      #+begin_src jupyter-python
      P = bigrams.float()
      P = P / P.sum(1) # sums rows
      #             ^--------------------- bug here: dimension will be (27,)
      #+end_src

      =P.shape= will be $(27, 27)$, while the sum will be a vector of length 27, i.e. shape $(27, )$. Therefore, according to the broadcasting rules, the broadcasted operation will divide the probabilities of each row across each column!
      #+begin_example
      P.shape        27 27
      row sum.shape     27
      #+end_example

** Naive way
#+begin_src jupyter-python
bigram_probs = bigrams.float()
bigram_probs = bigram_probs / bigram_probs.sum(1, keepdim=True)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
def generate_name (bigram_probs, rng):
    name = []
    idx = 0
    while True:
        p = bigram_probs[idx]
        idx = torch.multinomial(p, num_samples = 1, replacement = True, generator = rng).item()

        if idx == 0:
            break

        name += char_map[idx]

    return ''.join(name)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
rng = torch.Generator().manual_seed((2 << 30) - 1)

num_names = 10
print([generate_name(bigram_probs, rng) for _ in range(num_names)])
#+end_src

#+RESULTS:
: ['junide', 'janasah', 'p', 'cony', 'a', 'nn', 'kohin', 'tolian', 'juee', 'ksahnaauranilevias']

* Modelling Loss
Earlier, we had a straightforward way of modelling loss, since we had a test and train dataset that had numerical values. But how do we do that with language, where there are no numbers?

We can think of our model as a predictor, and what we want is the predictions to be as close as possible to the actual thing. How do we predict things? Using the probabiliies. Infact, the probability of generating a certain word is merely the product of the probabilities of the respective bigrams. This is basically a Maximum Likliehood Estimator in disguise.
$$
P(\text{word}) = \prod_{n = 0}^{\text{length}} P(\text{bigram})
$$

We know that we want the probability of generation to be as close to 1 as possible, since that way it would generate only that word. A naive loss function, therefore, would merely be the product. We can also take the negative log likelihood (the negative is merely to turn the function from negative to positive). However, this has a slight disadvantage: it penalises long names. To counter this, one can take the average loss per character.

#+begin_src jupyter-python
def loss_function (bigram_probs, word):
    word = [DEMARKATOR] + list(word) + [DEMARKATOR]
    log_likelihood = 0.0
    for ch1, ch2 in zip(word, word[1:]):
        log_likelihood += torch.log(bigram_probs[idx_map[ch1], idx_map[ch2]])
    return -log_likelihood / (len(word) - 1)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
loss_function(bigram_probs, generate_name(bigram_probs, rng))
#+end_src

#+RESULTS:
: tensor(2.8431)

** Fixing a minor problem
Here, there are some values with zero counts. This is dangerous for our loss function, because it might blow up the log function to =-inf=!
#+begin_src jupyter-python
torch.log(torch.tensor(0.0))
#+end_src

#+RESULTS:
: tensor(-inf)

In fact, it is easy to see this using some weird bigrams, like =qq=.
#+begin_src jupyter-python
loss_function(bigram_probs, "qq")
#+end_src

#+RESULTS:
: tensor(inf)

The answer: smoothen all values! We can add some small value to begin with, and count all occurences using that. This, as it turns out, is also known as [[https://en.wikipedia.org/wiki/Additive_smoothing][additive, or Laplacian smoothening]].
#+begin_src jupyter-python
smooth_bigram_probs = (bigrams + 1e-6).float()
smooth_bigram_probs = smooth_bigram_probs / smooth_bigram_probs.sum(1, keepdim=True)

loss_function(smooth_bigram_probs, "qq")
#+end_src

#+RESULTS:
: tensor(9.1825)

Note: This is basically creating a problem and fixing it, patting oneself on the back while doing so. Bayesian methods would make this update pointless, since every point always has some probability, be it however small.
* Chucking an NN at the problem. Problem solved?
Let's find out, shall we? :)
** Finding all bigrams
#+begin_src jupyter-python
def get_bigrams (word):
    word = [DEMARKATOR] + list(word) + [DEMARKATOR]
    for bigram in zip(word, word[1:]):
        yield bigram
#+end_src

#+RESULTS:

** Creating a one-hot encoding
#+begin_src jupyter-python
bigram_idxs = [[idx_map[ch] for ch in bigram] for word in words[:4] for bigram in get_bigrams(word)]
datapoints = list(zip(*bigram_idxs))
#+end_src

#+RESULTS:

#+begin_src jupyter-python
import torch.nn.functional as F

# converts a vector into a one-hot encoding
one_hot_encoding = lambda x: F.one_hot(torch.tensor(x), num_classes=len(LANGUAGE)).float() 

x_encoding, y_encoding = [one_hot_encoding(l) for l in datapoints]
#+end_src

#+RESULTS:

#+begin_src jupyter-python
x_encoding.shape
#+end_src

#+RESULTS:
: torch.Size([25, 27])

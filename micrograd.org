#+TITLE: Micrograd [[[https://www.youtube.com/watch?v=VMj-3S1tku0][YT Video]]]
#+DESCRIPTION:  The spelled-out intro to neural networks and backpropagation: building micrograd
#+PROPERTY: header-args:jupyter-python :session micrograd :kernel python3 :async yes

* Environment setup

#+begin_src shell :results verbatim
poetry add matplotlib
poetry add numpy
#+end_src

#+RESULTS:
#+begin_example
The following packages are already present in the pyproject.toml and will be skipped:

  • matplotlib

If you want to update it to the latest compatible version, you can use `poetry update package`.
If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.

Nothing to add.
The following packages are already present in the pyproject.toml and will be skipped:

  • numpy

If you want to update it to the latest compatible version, you can use `poetry update package`.
If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.

Nothing to add.
#+end_example

* Required Imports
#+begin_src jupyter-python
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline
#+end_src

#+RESULTS:

* Value Object
This will store each value of a neural network.

#+begin_src jupyter-python
class Value:
    def __init__ (self, data: float, label = '', grad = 0.0, _children = (), _op = ''):
        self.data = data
        self.label = label
        self.grad = grad
        self._prev = set(_children)
        self._op = _op

        self._backward = lambda: None

    def __repr__ (self):
        return f"(Value {self.label} data = {self.data})"
    
    def __add__ (self, other):
        other = other if isinstance(other, Value) else Value(other, label = str(other))
        out = Value(self.data + other.data,
                    _children = (self, other), _op = '+')

        def _backward():
            self.grad += out.grad
            other.grad += out.grad

        out._backward = _backward
        return out

    def __neg__ (self, other):
        return self * -1

    def __sub__ (self, other):
        return self + (- other)

    def __radd__ (self, other):
        return self + other

    def __rsub__ (self, other):
        return other + (- self)
    
    def __mul__ (self, other):
        other = other if isinstance(other, Value) else Value(other, label = str(other))
        out = Value(self.data * other.data,
                    _children = (self, other), _op = '*')

        def _backward():
            self.grad += other.data * out.grad
            other.grad += self.data * out.grad

        out._backward = _backward

        return out

    def __rmul__ (self, other):
        return self * other

    def exp (self):
        x = self.data
        out = Value(np.exp(x), _op = 'exp')

        def _backward():
            self.grad += out.data * out.grad

        out._backward = _backward

        return out

    def __pow__ (self, other):
        assert isinstance(other, (int, float))

        out = Value(self.data ** other, 
                    _children = (self,), _op = 'pow')

        def _backward ():
            self.grad += (other * self.data ** (other - 1)) * out.grad

        out._backward = _backward
        return out

    def __truediv__ (self, other):
        return self * (other ** -1)

    def __rtruediv__ (self, other):
        return other * (self ** -1)

    def tanh (self):
        x = self.data
        t = (np.exp(2 * x) - 1) / (np.exp(2 * x) + 1)
        out = Value(t, _children = (self, ), _op = 'tanh')

        def _backward():
            self.grad = (1 - t ** 2) * out.grad

        out._backward = _backward
        return out

    def backward (self):
        topo = []
        visited = set()

        def build_topo (v):
            if v not in visited:
                visited.add(v)
                for child in v._prev:
                    build_topo(child)
                topo.append(v)

        build_topo(self)

        self.grad = 1.0
        for node in reversed(topo):
            node._backward()
#+end_src

#+RESULTS:


#+begin_src jupyter-python
a = Value(0.1, label = 'a')
b = Value(2, label = 'b')
c = Value(0.3, label = 'c')
d = (a + b + c)
e = (2 * d).exp()
f = (e - 1) / (e + 1)

f.backward()
draw(f)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/525f358e504abbe1e0fb27eae1f9d422755a7810.svg]]

#+begin_src jupyter-python
g = d.tanh()
g.backward()
draw(g)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/15945a14d7bca7cb52927f96e7ad25852be207fe.svg]]

** Gradient Descent
Let us assume $L = a + b \cdot c$. Let us assume that $a$, $b$ and $c$ are independent values that can be computed.

What happens to $L$ if we perturb $a$? This would be equivalent to finding the partial derivative of $L$ with respect to $a$, since we want to oberserve the change occuring with respect to the change in $a$. We can compute it in a straightforward manner:
$$
\frac{\partial L}{\partial a} = 1
$$

Again, what happens if we perturb $b$? Again, this would be the same as the partial derivative of $L$ with respect to $b$.
$$
\frac{\partial L}{\partial b} = c,
$$
and similarly for $c$.

Now, what if $b$ itself was not an independent value? Let us say that $b = e \cdot f$, where $e$ and $f$ are independent variables. Now, computing
$$
\frac{\partial L}{\partial e}
$$

is slightly challenging, but we can simplify our work greatly using the chain rule. We know
$$
\frac{\partial L}{\partial b}
$$
from our earlier calculations, and we also know
$$
\frac{\partial b}{\partial e} = f
$$

From the chain rule, we can calculate
$$
\frac{\partial L}{\partial e} = \frac{\partial L}{\partial b} \cdot \frac{\partial b}{\partial e} = c \cdot f,
$$
which allows us to decompose the calculation of the derivative easily.
* Visualising a function
We use graphviz to visualise a function. We first install graphviz:

** GraphViz Installation
#+begin_src shell :results verbatim
poetry add graphviz 
#+end_src

#+RESULTS:
: The following packages are already present in the pyproject.toml and will be skipped:
: 
:   â¢ graphviz
: 
: If you want to update it to the latest compatible version, you can use `poetry update package`.
: If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.
: 
: Nothing to add.
#+begin_src jupyter-python
from graphviz import Digraph
#+end_src

#+RESULTS:

** Tracing a Value across children
To visualise the computation, we first require building the entire graph so that we can traverse and build it. This can be done using a simple depth first search that visits every node, includes the node if it hasn't already found it and continues along its merry way.
#+begin_src jupyter-python
def trace (root: Value):
    nodes = set()
    edges = set()

    def build (v: Value):
        if v not in nodes:
            nodes.add(v)
            for child in v._prev:
                edges.add((child, v))
                build(child)

    build(root)
    
    return nodes, edges
#+end_src

#+RESULTS:
** Actually building the graph
#+begin_src jupyter-python
def draw (root: Value):
    dot = Digraph(format = 'svg', graph_attr = {'rankdir': 'LR'})

    nodes, edges = trace(root)

    for n in nodes:
        uid = str(id(n))

        # Creates a rectangular node
        dot.node(name = uid, label = f"{n.label} | data {n.data} | grad {n.grad}", shape = 'record')

        # If the value is a result of some operation,
        # it creates a new node with that operation and connects
        # the element to that operation
        if n._op:
            dot.node(name = uid + n._op, label = n._op)
            dot.edge(uid + n._op, uid)

    # Connects all edges (computations) together
    for u, v in edges:
        dot.edge(str(id(u)), str(id(v)) + v._op)

    return dot
#+end_src

#+RESULTS:

** Testing
#+begin_src jupyter-python
draw(a + b * c)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/bbecbd9eb6ae0cc3c902c8cc5c43618c08a01888.svg]]

* Building a Multi Layer Perceptron
** A Neuron
A neuron can be considered as a function, that takes in some inputs, weights them according to some internal parameters (aptly named weights), and returns the weighted sum. The most important bit of the neural network is the non-linearity that is applied to it, which allows neural networks to become universal function approximators.

#+begin_src jupyter-python
class Neuron:
    def __init__ (self, num_in):
        self.weights = [Value(np.random.rand()) for _ in range(num_in)]
        self.bias = np.random.rand()

    def __call__ (self, x):
        assert len(x) == len(self.weights), "number of inputs is less than number of weights"
        aggregate = sum((w_i * x_i for (w_i, x_i) in zip(self.weights, x)), self.bias)
        print(aggregate)
        out = aggregate.tanh()
        return out
#+end_src

#+RESULTS:


#+begin_src jupyter-python
x = [Value(x) for x in [1.0, 2.0]]
n = Neuron(len(x))

n(x)
#+end_src

#+RESULTS:
:RESULTS:
: (Value ((() * ()) + (0.8800419654998817)) + (() * ()) data = 2.2007851933244797)
| Value | tanh | (((nil * nil) + (0.8800419654998817)) + (nil * nil)) | data | = | 0.9757807318755207 |
:END:
** A Layer
A layer is nothing but a list of neurons put together.
#+begin_src jupyter-python
class Layer:
    def __init__ (self, num_neurons, num_in):
        self.neurons = [Neuron(num_in) for _ in range(num_neurons)]

    def __call__ (self, x):
        outs = [n(x) for n in self.neurons]
        return outs[0] if len(outs) == 1 else outs
#+end_src

#+RESULTS:

#+begin_src jupyter-python
x = [Value(x) for x in [1.0, 2.0]]
l = Layer(3d, 2)

l(x)
#+end_src

#+RESULTS:
:RESULTS:
: (Value ((() * ()) + (0.31080234265130624)) + (() * ()) data = 1.2967529577606478)
: (Value ((() * ()) + (0.34570931464162247)) + (() * ()) data = 2.3388777837005033)
: (Value ((() * ()) + (0.651009990883962)) + (() * ()) data = 2.672056163019904)
| Value | tanh | (((nil * nil) + (0.31080234265130624)) + (nil * nil)) | data | = |   0.86088492035997 |
| Value | tanh | (((nil * nil) + (0.34570931464162247)) + (nil * nil)) | data | = | 0.9815716543031633 |
| Value | tanh | (((nil * nil) + (0.651009990883962)) + (nil * nil))   | data | = |  0.990493029044745 |
:END:
** Finally, an MLP
#+begin_src jupyter-python
class MLP:
    def __init__ (self, layer_neurons, num_in):
        num_ins = [num_in] + layer_neurons[:-1]
        self.layers = [Layer(*l) for l in zip(layer_neurons, num_ins)]

    def __call__ (self, x):
        for layer in self.layers:
            x = layer(x)
        return x
#+end_src

#+RESULTS:

#+begin_src jupyter-python
x = [2.0, 3.0, -1.0]
mlp = MLP([4, 5, 2, 1], len(x))

mlp(x)
draw(mlp(x))
#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
(Value  data = 2.2181667211490605)
(Value  data = 2.58186679728638)
(Value  data = 2.95802137383244)
(Value  data = 3.326915816594294)
(Value  data = 3.3253621724309514)
(Value  data = 2.5611706548939726)
(Value  data = 2.68057621760593)
(Value  data = 2.111074444188271)
(Value  data = 1.9257716022167577)
(Value  data = 3.6397028343865143)
(Value  data = 2.317410899246514)
(Value  data = 1.8536479322250843)
(Value  data = 2.2181667211490605)
(Value  data = 2.58186679728638)
(Value  data = 2.95802137383244)
(Value  data = 3.326915816594294)
(Value  data = 3.3253621724309514)
(Value  data = 2.5611706548939726)
(Value  data = 2.68057621760593)
(Value  data = 2.111074444188271)
(Value  data = 1.9257716022167577)
(Value  data = 3.6397028343865143)
(Value  data = 2.317410899246514)
(Value  data = 1.8536479322250843)
#+end_example
[[file:./.ob-jupyter/388957364149ba2f030ea35584dc1b27650f9c8b.svg]]
:END:

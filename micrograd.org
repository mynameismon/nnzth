#+TITLE: Micrograd [[[https://www.youtube.com/watch?v=VMj-3S1tku0][YT Video]]]
#+DESCRIPTION:  The spelled-out intro to neural networks and backpropagation: building micrograd
#+PROPERTY: header-args:jupyter-python :session micrograd :kernel python3 :async yes

* Environment setup

#+begin_src shell :results verbatim
poetry add matplotlib
poetry add numpy
#+end_src

#+RESULTS:
#+begin_example
The following packages are already present in the pyproject.toml and will be skipped:

  • matplotlib

If you want to update it to the latest compatible version, you can use `poetry update package`.
If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.

Nothing to add.
The following packages are already present in the pyproject.toml and will be skipped:

  • numpy

If you want to update it to the latest compatible version, you can use `poetry update package`.
If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.

Nothing to add.
#+end_example

* Required Imports
#+begin_src jupyter-python
import matplotlib.pyplot as plt
import numpy as np

%matplotlib inline
#+end_src

#+RESULTS:

* Value Object
This will store each value of a neural network.

#+begin_src jupyter-python
class Value:
    def __init__ (self, data: float, label = '', grad = 0.0, _children = (), _op = ''):
        self.data = data
        self.label = label
        self.grad = grad
        self._prev = set(_children)
        self._op = _op

        self._backward = lambda: None

    def __repr__ (self):
        return f"(Value {self.label} data = {self.data})"
    
    def __add__ (self, other):
        other = other if isinstance(other, Value) else Value(other, label = str(other))
        out = Value(self.data + other.data,
                    label = f"({self.label}) + ({other.label})",
                    _children = (self, other), _op = '+')

        def _backward():
            self.grad += out.grad
            other.grad += out.grad

        out._backward = _backward
        return out

    def __neg__ (self, other):
        return self * -1

    def __sub__ (self, other):
        return self + (- other)

    def __radd__ (self, other):
        return self + other

    def __rsub__ (self, other):
        return other + (- self)
    
    def __mul__ (self, other):
        other = other if isinstance(other, Value) else Value(other, label = str(other))
        out = Value(self.data * other.data,
                    label = f"({self.label}) * ({other.label})",
                    _children = (self, other), _op = '*')

        def _backward():
            self.grad += other.data * out.grad
            other.grad += self.data * out.grad

        out._backward = _backward

        return out

    def __rmul__ (self, other):
        return self * other

    def exp (self):
        x = self.data
        out = Value(np.exp(x), label = f"exp({self.label})", _children = (self, ), _op = 'exp')

        def _backward():
            self.grad += out.data * out.grad

        out._backward = _backward

        return out

    def __pow__ (self, other):
        assert isinstance(other, (int, float))

        out = Value(self.data ** other, label = f"(({self.label})^{other})",
                    _children = (self,), _op = 'pow')

        def _backward ():
            self.grad += (other * self.data ** (other - 1)) * out.grad

        out._backward = _backward
        return out

    def __truediv__ (self, other):
        return self * (other ** -1)

    def __rtruediv__ (self, other):
        return other * (self ** -1)

    def tanh (self):
        x = self.data
        t = (np.exp(2 * x) - 1) / (np.exp(2 * x) + 1)
        out = Value(t, label = f"tanh({self.label})", _children = (self, ), _op = 'tanh')

        def _backward():
            self.grad = (1 - t ** 2) * out.grad

        out._backward = _backward
        return out

    def backward (self):
        topo = []
        visited = set()

        def build_topo (v):
            if v not in visited:
                visited.add(v)
                for child in v._prev:
                    build_topo(child)
                topo.append(v)

        build_topo(self)

        self.grad = 1.0
        for node in reversed(topo):
            node._backward()
#+end_src

#+RESULTS:


#+begin_src jupyter-python
a = Value(0.1, label = 'a')
b = Value(2, label = 'b')
c = Value(0.3, label = 'c')
d = (a + b + c)
e = (2 * d).exp()
f = (e - 1) / (e + 1)

f.backward()
draw(f)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/67a146117de4fa87dab7d45a219ab01baf4021c1.svg]]

#+begin_src jupyter-python
g = d.tanh()
g.backward()
draw(g)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/15945a14d7bca7cb52927f96e7ad25852be207fe.svg]]

** Gradient Descent
Let us assume $L = a + b \cdot c$. Let us assume that $a$, $b$ and $c$ are independent values that can be computed.

What happens to $L$ if we perturb $a$? This would be equivalent to finding the partial derivative of $L$ with respect to $a$, since we want to oberserve the change occuring with respect to the change in $a$. We can compute it in a straightforward manner:
$$
\frac{\partial L}{\partial a} = 1
$$

Again, what happens if we perturb $b$? Again, this would be the same as the partial derivative of $L$ with respect to $b$.
$$
\frac{\partial L}{\partial b} = c,
$$
and similarly for $c$.

Now, what if $b$ itself was not an independent value? Let us say that $b = e \cdot f$, where $e$ and $f$ are independent variables. Now, computing
$$
\frac{\partial L}{\partial e}
$$

is slightly challenging, but we can simplify our work greatly using the chain rule. We know
$$
\frac{\partial L}{\partial b}
$$
from our earlier calculations, and we also know
$$
\frac{\partial b}{\partial e} = f
$$

From the chain rule, we can calculate
$$
\frac{\partial L}{\partial e} = \frac{\partial L}{\partial b} \cdot \frac{\partial b}{\partial e} = c \cdot f,
$$
which allows us to decompose the calculation of the derivative easily.
* Visualising a function
We use graphviz to visualise a function. We first install graphviz:

** GraphViz Installation
#+begin_src shell :results verbatim
poetry add graphviz 
#+end_src

#+RESULTS:
: The following packages are already present in the pyproject.toml and will be skipped:
: 
:   â¢ graphviz
: 
: If you want to update it to the latest compatible version, you can use `poetry update package`.
: If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.
: 
: Nothing to add.
#+begin_src jupyter-python
from graphviz import Digraph
#+end_src

#+RESULTS:

** Tracing a Value across children
To visualise the computation, we first require building the entire graph so that we can traverse and build it. This can be done using a simple depth first search that visits every node, includes the node if it hasn't already found it and continues along its merry way.
#+begin_src jupyter-python
def trace (root: Value):
    nodes = set()
    edges = set()

    def build (v: Value):
        if v not in nodes:
            nodes.add(v)
            for child in v._prev:
                edges.add((child, v))
                build(child)

    build(root)
    
    return nodes, edges
#+end_src

#+RESULTS:
** Actually building the graph
#+begin_src jupyter-python
def draw (root: Value):
    dot = Digraph(format = 'svg', graph_attr = {'rankdir': 'LR'})

    nodes, edges = trace(root)

    for n in nodes:
        uid = str(id(n))

        # Creates a rectangular node
        dot.node(name = uid, label = f"{n.label} | data {n.data} | grad {n.grad}", shape = 'record')

        # If the value is a result of some operation,
        # it creates a new node with that operation and connects
        # the element to that operation
        if n._op:
            dot.node(name = uid + n._op, label = n._op)
            dot.edge(uid + n._op, uid)

    # Connects all edges (computations) together
    for u, v in edges:
        dot.edge(str(id(u)), str(id(v)) + v._op)

    return dot
#+end_src

#+RESULTS:

** Testing
#+begin_src jupyter-python
draw(a + b * c)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/bbecbd9eb6ae0cc3c902c8cc5c43618c08a01888.svg]]
